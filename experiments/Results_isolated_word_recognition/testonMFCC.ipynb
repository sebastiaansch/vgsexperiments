{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tables\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/sebastiaanscholten/Documents/speech2image-master/PyTorch/functions')\n",
    "\n",
    "from trainer import flickr_trainer\n",
    "from encoders import img_encoder, audio_rnn_encoder\n",
    "from data_split import split_data_flickr\n",
    "from minibatchers import iterate_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class personaltrainer(flickr_trainer):\n",
    "    def audio_batcher(self, data, batch_size, shuffle):\n",
    "        return iterate_audio(data, batch_size, self.vis, self.cap, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-data_loc DATA_LOC] [-split_loc SPLIT_LOC]\n",
      "                             [-results_loc RESULTS_LOC]\n",
      "                             [-batch_size BATCH_SIZE] [-cuda CUDA]\n",
      "                             [-visual VISUAL] [-cap CAP]\n",
      "                             [-gradient_clipping GRADIENT_CLIPPING]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/sebastiaanscholten/Library/Jupyter/runtime/kernel-43ed1585-42fb-4907-836d-86b9ce0e4eb5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiaanscholten/opt/anaconda3/envs/merkxmodel/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Create and run an articulatory feature classification DNN')\n",
    "\n",
    "# args concerning file location\n",
    "parser.add_argument('-data_loc', type = str, default = '/Users/sebastiaanscholten/Documents/speech2image-master/preprocessing/prep_data/flickr_features_27jan_fixed.h5',\n",
    "                    help = 'location of the feature file, default: /prep_data/flickr_features.h5')\n",
    "parser.add_argument('-split_loc', type = str, default = '/Users/sebastiaanscholten/Documents/speech2image-master/preprocessing/testfolder2/test/dataset.json',\n",
    "                    help = 'location of the json file containing the data split information')\n",
    "parser.add_argument('-results_loc', type = str, default = '/Users/sebastiaanscholten/Documents/speech2image-master/PyTorch/flickr_audio/results/',\n",
    "                    help = 'location of the json file containing the data split information')\n",
    "# args concerning training settings\n",
    "parser.add_argument('-batch_size', type = int, default = 10, help = 'batch size, default: 100')\n",
    "parser.add_argument('-cuda', type = bool, default = False, help = 'use cuda, default: True')\n",
    "# args concerning the database and which features to load\n",
    "parser.add_argument('-visual', type = str, default = 'resnet', help = 'name of the node containing the visual features, default: resnet')\n",
    "parser.add_argument('-cap', type = str, default = 'mfcc', help = 'name of the node containing the audio features, default: mfcc')\n",
    "parser.add_argument('-gradient_clipping', type = bool, default = True, help ='use gradient clipping, default: True')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# create config dictionaries with all the parameters for your encoders\n",
    "\n",
    "audio_config = {'conv':{'in_channels': 39, 'out_channels': 64, 'kernel_size': 6, 'stride': 2,\n",
    "               'padding': 0, 'bias': False}, 'rnn':{'input_size': 64, 'hidden_size': 1024, \n",
    "               'num_layers': 4, 'batch_first': True, 'bidirectional': True, 'dropout': 0}, \n",
    "               'att':{'in_size': 2048, 'hidden_size': 128, 'heads': 1}}\n",
    "# automatically adapt the image encoder output size to the size of the caption encoder\n",
    "out_size = audio_config['rnn']['hidden_size'] * 2**audio_config['rnn']['bidirectional'] * audio_config['att']['heads']\n",
    "image_config = {'linear':{'in_size': 2048, 'out_size': out_size}, 'norm': True}\n",
    "\n",
    "\n",
    "# open the data file\n",
    "data_file = tables.open_file(args.data_loc, mode='r+') \n",
    "\n",
    "# check if cuda is availlable and user wants to run on gpu\n",
    "cuda = args.cuda and torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('using gpu')\n",
    "else:\n",
    "    print('using cpu')\n",
    "\n",
    "# flickr doesnt need to be split at the root node\n",
    "def iterate_data(h5_file):\n",
    "    for x in h5_file.root:\n",
    "        yield x\n",
    "f_nodes = [node for node in iterate_data(data_file)]\n",
    "    \n",
    "# split the database into train test and validation sets. default settings uses the json file\n",
    "# with the karpathy split\n",
    "test = f_nodes[0:100]\n",
    "#####################################################\n",
    "\n",
    "# network modules\n",
    "img_net = img_encoder(image_config)\n",
    "cap_net = audio_rnn_encoder(audio_config)\n",
    "\n",
    "# list all the trained model parameters\n",
    "models = os.listdir(args.results_loc)\n",
    "caption_models = [x for x in models if 'caption' in x]\n",
    "img_models = [x for x in models if 'image' in x]\n",
    "\n",
    "# run the image and caption retrieval\n",
    "img_models.sort()\n",
    "caption_models.sort()\n",
    "\n",
    "# create a trainer with just the evaluator for the purpose of testing a pretrained model\n",
    "trainer = personaltrainer(img_net, cap_net, args.visual, args.cap)\n",
    "trainer.set_audio_batcher()\n",
    "# optionally use cuda\n",
    "if cuda:\n",
    "    trainer.set_cuda()\n",
    "trainer.set_evaluator([1, 5, 10])\n",
    "\n",
    "for img, cap in zip(img_models, caption_models):\n",
    "\n",
    "    epoch = img.split('.')[1]\n",
    "    # load the pretrained embedders\n",
    "    trainer.load_cap_embedder(args.results_loc + cap)\n",
    "    trainer.load_img_embedder(args.results_loc + img)\n",
    "    \n",
    "    # calculate the recall@n\n",
    "    trainer.set_epoch(epoch)\n",
    "    trainer.recall_at_n(test, args.batch_size, prepend='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
